{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
    "``autograd`` to define models and differentiate them.\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
    "returns the ``output``.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    ".. figure:: /_static/img/mnist.png\n",
    "   :alt: convnet\n",
    "\n",
    "   convnet\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Define the network\n",
    "------------------\n",
    "\n",
    "Let’s define this network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "[Parameter containing:\n",
      "tensor([[[[-0.1979,  0.1013,  0.0348,  0.1187,  0.1795],\n",
      "          [ 0.1467, -0.0622,  0.0115,  0.0726,  0.1335],\n",
      "          [-0.0673,  0.1585,  0.0749,  0.0385,  0.1334],\n",
      "          [ 0.1109,  0.1758,  0.1782, -0.0970,  0.1438],\n",
      "          [ 0.0411, -0.1844,  0.1008,  0.1439,  0.0455]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1915, -0.1639,  0.1268, -0.1267,  0.1584],\n",
      "          [-0.0299,  0.0200,  0.0247, -0.0909, -0.1463],\n",
      "          [-0.0877, -0.1304,  0.0484, -0.1930,  0.1543],\n",
      "          [ 0.1761,  0.1786, -0.1769,  0.1785, -0.0825],\n",
      "          [-0.1275, -0.0643, -0.1687,  0.0923, -0.0913]]],\n",
      "\n",
      "\n",
      "        [[[-0.0954,  0.1215, -0.0013, -0.0320,  0.1484],\n",
      "          [-0.1354, -0.1959,  0.0388, -0.1843, -0.1799],\n",
      "          [-0.0865, -0.1660,  0.0106, -0.0824,  0.0014],\n",
      "          [ 0.1025,  0.0461, -0.0328,  0.1809,  0.0811],\n",
      "          [-0.0247, -0.0873, -0.1576,  0.1619,  0.1878]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1586,  0.1471, -0.1886,  0.0569,  0.1914],\n",
      "          [-0.0480, -0.0924, -0.1301, -0.1920, -0.0129],\n",
      "          [ 0.0689,  0.0423, -0.0573, -0.1638, -0.1441],\n",
      "          [ 0.1189,  0.0314, -0.0192,  0.1622,  0.1196],\n",
      "          [ 0.1452, -0.0344,  0.0446, -0.1317, -0.0872]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0351, -0.0703, -0.1704, -0.0823,  0.0299],\n",
      "          [-0.1723,  0.0375, -0.0042,  0.1746,  0.1037],\n",
      "          [ 0.1621,  0.0766, -0.0215,  0.1557, -0.1921],\n",
      "          [-0.0994, -0.0482, -0.0280, -0.1599,  0.0615],\n",
      "          [ 0.0575,  0.0400, -0.0128,  0.0292,  0.0470]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1695,  0.1259, -0.1019,  0.0426, -0.1857],\n",
      "          [-0.0905, -0.0687, -0.0138,  0.1363, -0.1499],\n",
      "          [-0.0532, -0.1113, -0.1065, -0.1659, -0.1364],\n",
      "          [-0.0006, -0.0578,  0.0918,  0.1527,  0.1558],\n",
      "          [-0.0940,  0.0301, -0.1905,  0.1865, -0.0063]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0755,  0.0826, -0.1228, -0.0961,  0.0394, -0.1375],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 3.1297e-02,  2.4962e-02,  5.5348e-02,  6.8334e-02, -6.2532e-03],\n",
      "          [-5.8916e-02, -1.5115e-02,  4.2660e-03,  3.0598e-02, -5.0713e-02],\n",
      "          [ 6.6031e-02,  7.4576e-02,  6.9698e-02,  7.5755e-04, -7.5180e-02],\n",
      "          [-7.1966e-02, -7.3618e-02, -4.4462e-02,  2.1702e-02, -7.8338e-02],\n",
      "          [-6.3303e-02, -7.2660e-02, -4.9157e-02,  5.3841e-02, -7.0939e-02]],\n",
      "\n",
      "         [[-3.5603e-02,  7.0679e-02, -2.1066e-02,  6.1984e-02, -3.5001e-02],\n",
      "          [ 5.1711e-02, -6.3999e-02, -3.0528e-02, -3.6657e-02, -5.4886e-03],\n",
      "          [-7.2438e-02, -5.5975e-02,  8.8261e-03,  3.6735e-02, -8.1397e-02],\n",
      "          [-5.1031e-02, -1.5096e-02, -6.0305e-02,  7.8605e-02, -2.1063e-03],\n",
      "          [-4.0083e-02, -4.2942e-03,  4.9223e-02,  3.8723e-03, -5.2191e-02]],\n",
      "\n",
      "         [[ 1.7827e-02,  6.6094e-03,  3.0630e-02,  7.8469e-02, -2.1822e-02],\n",
      "          [ 4.6306e-02, -2.3964e-02, -8.2948e-03,  6.1276e-02, -2.5541e-02],\n",
      "          [-8.1310e-02, -6.6734e-02, -5.9836e-02, -1.7076e-02,  2.4344e-02],\n",
      "          [-3.5847e-02,  6.6282e-02,  2.6100e-02,  3.1526e-02,  2.2847e-02],\n",
      "          [ 1.7521e-02, -6.9293e-02, -8.7968e-03, -4.0713e-03,  3.6216e-02]],\n",
      "\n",
      "         [[-7.8476e-03,  3.4449e-02,  5.2576e-02,  4.7323e-02,  3.5715e-02],\n",
      "          [-5.9949e-02, -4.6355e-02,  1.3834e-02,  2.8940e-02, -3.4197e-02],\n",
      "          [-1.6751e-02, -5.8167e-03,  3.4706e-02, -5.3106e-02, -5.0357e-02],\n",
      "          [ 2.5198e-02,  4.7988e-02,  6.3722e-02, -4.7597e-02,  6.3166e-02],\n",
      "          [ 4.9801e-02,  2.4679e-02,  4.4476e-02,  5.3284e-02, -6.7850e-02]],\n",
      "\n",
      "         [[-1.6800e-03, -4.1580e-03, -1.6538e-02,  7.5892e-02,  1.3992e-02],\n",
      "          [-1.8566e-03, -7.6415e-02,  7.0213e-02, -6.7489e-02,  5.0583e-02],\n",
      "          [ 3.2706e-02,  5.6951e-02, -2.5198e-02,  5.2600e-02,  5.7508e-02],\n",
      "          [-7.0431e-02, -4.5611e-02,  7.6687e-02,  2.7788e-02,  8.7688e-03],\n",
      "          [ 5.7113e-02, -7.0415e-02,  1.7005e-02,  7.6076e-02,  2.6763e-02]],\n",
      "\n",
      "         [[ 4.4245e-02,  6.0561e-02,  5.5358e-04,  5.2916e-02,  2.8909e-02],\n",
      "          [ 5.9466e-02,  9.4914e-03,  7.7461e-02,  6.6535e-02, -4.3550e-02],\n",
      "          [-6.9401e-03,  7.5215e-02,  6.5758e-02,  8.4219e-03,  4.9422e-02],\n",
      "          [-1.4967e-02,  5.8700e-02, -2.2091e-02,  6.2951e-02,  2.0302e-03],\n",
      "          [-2.6929e-03,  7.7571e-02, -2.5929e-02,  1.2307e-02, -4.4497e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.7495e-02, -2.1405e-02, -5.4444e-02, -2.1543e-02,  7.3794e-02],\n",
      "          [ 2.6050e-03,  1.8051e-02,  1.0603e-02, -1.6534e-02,  7.6197e-02],\n",
      "          [ 3.7886e-02, -4.8284e-02, -5.7190e-02, -5.9795e-02, -2.1884e-02],\n",
      "          [-1.3427e-02,  4.7539e-02,  4.2449e-02,  6.2795e-02,  9.8253e-03],\n",
      "          [-9.7455e-03,  8.6523e-03,  5.5890e-02,  1.1825e-02,  5.0771e-02]],\n",
      "\n",
      "         [[ 2.6212e-02,  3.0083e-02, -1.9832e-02, -6.7965e-02,  5.7743e-02],\n",
      "          [-7.1229e-02,  4.9981e-02,  6.8822e-02, -7.0007e-02,  7.7213e-02],\n",
      "          [ 3.2943e-02, -8.0867e-02, -3.3153e-02,  6.6530e-02, -6.4268e-02],\n",
      "          [-3.3990e-02, -5.8084e-02, -3.8621e-02, -1.7806e-02,  6.1501e-02],\n",
      "          [-7.6798e-03, -3.2410e-02,  7.0373e-02, -3.0421e-02, -6.8168e-02]],\n",
      "\n",
      "         [[-4.8661e-02,  1.2336e-02,  2.6724e-02, -5.2173e-02,  1.6993e-02],\n",
      "          [ 6.1718e-02,  2.5789e-02, -5.1363e-02, -5.2593e-02, -7.1687e-02],\n",
      "          [-7.8150e-02,  4.5991e-02,  3.2569e-02, -3.5897e-02,  6.9792e-02],\n",
      "          [-4.9182e-02,  4.2026e-02,  6.6029e-02, -1.5830e-02, -4.5207e-02],\n",
      "          [ 6.7379e-02, -7.3012e-02,  6.9491e-02, -7.7898e-02,  2.2708e-02]],\n",
      "\n",
      "         [[ 5.1265e-02,  2.2980e-02, -6.9792e-02, -2.6527e-02, -8.1356e-02],\n",
      "          [ 3.9759e-02, -1.3801e-02,  1.1110e-02, -6.7617e-02, -2.6655e-02],\n",
      "          [-5.0503e-02,  7.5672e-02,  3.7737e-02, -5.6498e-02,  7.6175e-02],\n",
      "          [-6.5156e-02,  3.6389e-02, -5.5328e-03, -2.5002e-02,  2.8151e-02],\n",
      "          [-7.2351e-02,  2.3001e-02, -6.9167e-02, -7.9436e-02, -2.6570e-02]],\n",
      "\n",
      "         [[-5.4463e-02, -4.1862e-02,  1.8052e-02, -5.1000e-02, -7.9016e-02],\n",
      "          [-1.0527e-02,  7.6258e-02,  4.4934e-03,  7.8202e-02, -2.1491e-03],\n",
      "          [-3.7477e-02, -7.4928e-02, -4.9989e-02,  6.6215e-02,  1.8507e-02],\n",
      "          [ 4.0589e-02, -4.7429e-02, -3.6563e-02,  7.6486e-02, -1.8726e-02],\n",
      "          [-3.7447e-02, -1.3829e-02,  8.0750e-02,  7.4124e-02,  4.3142e-02]],\n",
      "\n",
      "         [[ 6.5682e-02,  1.6142e-02,  6.3869e-02, -3.9853e-02, -5.3545e-02],\n",
      "          [ 5.6127e-04,  3.0497e-02, -6.2733e-02, -8.0810e-02, -8.0134e-02],\n",
      "          [ 1.9928e-02,  3.1918e-04, -1.4400e-02,  5.6559e-02, -5.5900e-02],\n",
      "          [-4.0429e-02,  3.1262e-02,  3.5568e-02, -3.6526e-02, -6.1026e-02],\n",
      "          [ 5.7642e-02,  5.2582e-02, -1.3613e-02,  5.5908e-02, -6.5125e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5291e-02,  5.7066e-02, -1.9972e-02,  7.5621e-02, -7.5266e-02],\n",
      "          [-5.0975e-02, -2.0354e-02, -4.5228e-02,  3.4938e-02, -2.8894e-02],\n",
      "          [ 1.7056e-02,  4.7429e-02,  5.2195e-02,  2.3366e-02, -2.1940e-02],\n",
      "          [ 8.0994e-02, -5.8754e-02, -1.6380e-02, -5.4280e-02,  7.6072e-02],\n",
      "          [-6.3204e-02, -1.5557e-03,  5.8370e-02, -1.6362e-02,  3.9133e-02]],\n",
      "\n",
      "         [[-5.7315e-03,  7.5966e-02,  1.5490e-02,  8.1006e-02,  5.1981e-02],\n",
      "          [-5.2859e-02,  1.4471e-03,  7.3424e-02,  7.8690e-02, -4.1610e-02],\n",
      "          [ 4.2792e-03,  1.3278e-02, -2.2508e-02,  3.8784e-02, -4.0966e-02],\n",
      "          [-7.9183e-02,  6.6551e-03, -6.2636e-02, -5.3378e-02, -1.0648e-02],\n",
      "          [-1.1500e-02,  4.4923e-02,  3.9519e-02, -2.6790e-02,  7.3743e-02]],\n",
      "\n",
      "         [[-6.2824e-02, -3.8959e-02,  7.3125e-03, -4.9631e-02, -6.6089e-02],\n",
      "          [ 2.1073e-02,  1.5152e-02, -2.0424e-02,  1.5038e-02,  7.6125e-02],\n",
      "          [ 4.6134e-02,  2.6115e-02, -7.8963e-02,  7.4837e-03, -6.6107e-02],\n",
      "          [-2.5986e-02,  5.3049e-02,  6.5005e-02,  5.7126e-02,  1.1516e-03],\n",
      "          [ 7.7570e-02, -1.9030e-02, -4.8939e-02, -3.9966e-02, -5.7263e-02]],\n",
      "\n",
      "         [[ 5.2315e-02, -1.3189e-02,  7.5616e-02, -4.9907e-02,  6.5023e-02],\n",
      "          [-7.7588e-02, -6.1470e-02, -7.1774e-02, -2.0905e-02, -2.0592e-02],\n",
      "          [-4.7946e-02, -6.5241e-02, -2.5353e-02, -5.3671e-02,  4.2776e-02],\n",
      "          [-6.8552e-02,  5.4780e-02,  6.1155e-02, -7.4344e-02,  8.1589e-02],\n",
      "          [ 3.4148e-02, -2.9407e-02, -7.5265e-02, -3.1766e-02, -5.4119e-02]],\n",
      "\n",
      "         [[-4.0789e-02, -6.5610e-02, -7.9259e-02, -7.9186e-02,  2.2345e-02],\n",
      "          [ 5.5428e-02,  8.0959e-04, -4.8929e-03,  1.4944e-03,  2.4600e-03],\n",
      "          [-6.9497e-02,  3.5622e-02,  5.5808e-02,  4.1076e-02,  4.6258e-02],\n",
      "          [ 5.9388e-02, -3.7122e-02,  5.1004e-02, -1.7694e-02, -1.8829e-02],\n",
      "          [-2.2166e-02,  2.5581e-02, -5.9629e-03,  2.4536e-02,  4.3451e-03]],\n",
      "\n",
      "         [[-2.1854e-02, -1.2962e-02,  4.8879e-02, -2.2630e-02, -6.4403e-02],\n",
      "          [ 2.7312e-02,  1.8195e-02,  5.7502e-02,  1.1590e-02,  7.8677e-02],\n",
      "          [ 4.0759e-02,  6.1023e-02,  4.8820e-02, -1.6163e-02,  4.6219e-02],\n",
      "          [-5.3706e-02,  1.9214e-02, -5.1283e-02,  5.3068e-02, -8.0186e-02],\n",
      "          [ 1.9917e-02,  3.5949e-02, -2.9654e-02, -4.2783e-02,  6.8118e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.7672e-02,  5.4357e-02, -1.6935e-02, -7.1561e-02, -2.6717e-02],\n",
      "          [-4.5797e-02, -1.3753e-02, -4.4079e-02,  7.4738e-03, -2.9209e-02],\n",
      "          [-7.5043e-02,  6.1042e-02,  2.1742e-02, -1.9033e-02,  5.3007e-02],\n",
      "          [ 3.9981e-03, -6.6312e-03, -2.2786e-02, -9.1444e-03,  1.7546e-02],\n",
      "          [-3.9117e-02,  6.9837e-02, -7.4925e-02,  3.9078e-03,  4.3126e-03]],\n",
      "\n",
      "         [[-6.0773e-02,  7.4054e-02, -5.8884e-02,  8.1349e-02,  3.5909e-02],\n",
      "          [ 5.4014e-02,  7.1900e-02, -9.9757e-03,  6.7363e-03, -3.0288e-02],\n",
      "          [ 2.4929e-02,  2.2902e-02, -6.1734e-02,  5.1256e-02, -5.3276e-02],\n",
      "          [-6.6852e-02,  7.2649e-02, -6.4863e-02, -7.1632e-02, -4.8874e-03],\n",
      "          [-7.6692e-02, -7.8774e-02, -4.9159e-02,  6.9645e-02,  3.3279e-02]],\n",
      "\n",
      "         [[ 4.2596e-03, -6.1995e-02,  5.2863e-02, -1.9526e-02, -3.1727e-02],\n",
      "          [ 7.1258e-02, -7.0234e-02,  7.4758e-02,  4.2174e-02, -2.9545e-02],\n",
      "          [-2.7790e-03, -2.2943e-02, -8.0550e-02, -1.6708e-02, -3.4088e-02],\n",
      "          [ 6.4524e-02,  2.1502e-02, -6.7959e-02, -4.6603e-02, -3.8631e-02],\n",
      "          [-4.0778e-02,  1.5530e-02, -7.4540e-03, -2.2687e-02, -3.2800e-02]],\n",
      "\n",
      "         [[-7.9551e-02,  3.0134e-02, -3.5811e-02,  1.0696e-02, -6.7414e-02],\n",
      "          [-6.3424e-02,  4.1149e-02,  7.3431e-02,  4.3108e-02, -7.9885e-02],\n",
      "          [-4.0399e-02, -8.1005e-03,  6.5007e-02,  4.1209e-02, -5.5808e-02],\n",
      "          [ 6.8564e-02,  3.3080e-02, -4.9944e-02, -6.4807e-02, -6.3859e-02],\n",
      "          [-1.9963e-03,  6.5315e-02,  1.4929e-02, -4.1134e-02,  4.1675e-02]],\n",
      "\n",
      "         [[ 1.5236e-02,  3.1418e-02,  3.7456e-02,  2.7901e-02,  3.7288e-02],\n",
      "          [-1.4903e-02, -1.4054e-02, -1.0700e-02, -6.9215e-02, -2.1605e-02],\n",
      "          [ 7.7208e-02, -7.3100e-02, -4.8110e-02, -6.6034e-02,  6.1244e-02],\n",
      "          [ 3.0126e-02, -3.8035e-02,  3.9604e-02, -6.4464e-03, -1.3549e-02],\n",
      "          [-3.9593e-02,  5.2855e-03, -5.0172e-02, -4.4860e-03, -3.0579e-02]],\n",
      "\n",
      "         [[-2.5003e-02, -2.2874e-02,  1.4143e-02,  5.0974e-03,  1.7686e-02],\n",
      "          [ 6.8294e-02, -2.5112e-02,  4.2421e-02, -2.9706e-02, -2.6174e-02],\n",
      "          [ 5.6668e-02,  3.6017e-02, -3.1948e-02, -8.0409e-02,  7.1548e-04],\n",
      "          [-4.0468e-03,  6.2072e-02,  6.0931e-03,  1.6213e-02, -5.2999e-02],\n",
      "          [-2.8516e-02,  7.5528e-02,  3.4693e-02,  5.7627e-02,  5.4649e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.8996e-02,  1.8069e-02,  5.2629e-02,  7.2040e-02,  9.5582e-03],\n",
      "          [ 3.6249e-02, -3.5114e-02,  4.7665e-02,  5.3167e-02,  4.8264e-02],\n",
      "          [ 3.3214e-02,  4.4152e-02, -6.3305e-03,  5.6202e-02,  2.4546e-02],\n",
      "          [-7.8184e-02, -1.9123e-02,  2.3284e-02, -1.7853e-02, -2.0735e-02],\n",
      "          [ 1.5299e-02,  7.8429e-02,  8.7029e-04, -3.2981e-02,  7.1637e-02]],\n",
      "\n",
      "         [[ 7.2947e-02,  7.0884e-02,  5.5116e-02, -5.4469e-02, -1.3846e-02],\n",
      "          [ 5.2179e-02,  3.5321e-02,  3.3891e-03,  1.6508e-02, -1.2602e-02],\n",
      "          [ 6.5120e-02,  9.1002e-03,  6.9699e-02, -5.2287e-02, -3.3584e-02],\n",
      "          [ 7.1213e-02,  7.1236e-02,  1.4359e-02,  5.5897e-02, -1.4341e-02],\n",
      "          [-1.8838e-02, -6.7900e-02,  2.9701e-02, -5.6794e-03, -7.3362e-02]],\n",
      "\n",
      "         [[-3.6813e-02, -5.1997e-02,  1.2170e-02, -5.7921e-02,  2.6327e-02],\n",
      "          [-6.7568e-02,  4.8911e-02,  3.0054e-02, -8.0983e-02, -1.3380e-02],\n",
      "          [-5.0815e-02,  4.9883e-02, -4.7978e-02,  4.3375e-02, -1.7466e-02],\n",
      "          [-4.6280e-02, -7.9433e-02,  4.1059e-02, -6.9893e-02, -3.7425e-04],\n",
      "          [ 3.5298e-02,  9.5591e-05, -2.6583e-03, -2.0123e-02,  6.7421e-02]],\n",
      "\n",
      "         [[ 6.8941e-02, -1.9668e-02,  6.5552e-02,  1.1759e-02, -3.4405e-02],\n",
      "          [-6.4690e-02,  6.6579e-02,  5.4849e-02, -4.8287e-02,  6.5841e-02],\n",
      "          [-5.7029e-02, -4.0008e-02, -6.3686e-02,  7.8409e-02,  7.5260e-02],\n",
      "          [-7.8729e-02,  7.9237e-02,  4.0844e-02,  4.1617e-02, -1.9377e-02],\n",
      "          [ 5.0668e-02, -2.4028e-03,  1.8381e-02,  7.7274e-02,  4.4636e-02]],\n",
      "\n",
      "         [[ 4.6958e-02, -7.7897e-02,  8.0413e-02, -1.3825e-02, -4.9269e-02],\n",
      "          [ 5.6936e-02, -1.3450e-02, -4.6893e-02,  2.2972e-04, -2.3134e-03],\n",
      "          [ 2.3255e-02,  6.9592e-02, -5.6486e-02, -3.8008e-02, -1.2447e-02],\n",
      "          [ 6.6719e-02, -2.6470e-02,  5.1950e-02, -2.6652e-02,  3.1279e-02],\n",
      "          [ 4.2640e-03, -8.0142e-02, -3.4283e-02, -4.0552e-02, -1.0200e-02]],\n",
      "\n",
      "         [[ 7.4251e-03,  4.5186e-02,  6.8801e-02, -7.9058e-02,  4.0289e-02],\n",
      "          [-7.6730e-02, -7.3760e-03,  2.3376e-02,  2.0914e-03,  2.6693e-02],\n",
      "          [-2.6168e-03,  2.5404e-02, -5.7595e-02, -7.3520e-02,  4.9062e-02],\n",
      "          [-9.8353e-03,  7.0699e-02, -8.0451e-02, -7.7393e-02, -6.2429e-02],\n",
      "          [ 3.5510e-02, -4.9480e-02,  7.7613e-02,  7.4861e-04,  1.6189e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9300e-02,  1.2205e-02, -6.3968e-02,  7.6982e-03,  3.2483e-02],\n",
      "          [-5.9139e-03, -1.4628e-02, -3.9341e-04, -2.6521e-02,  2.4325e-02],\n",
      "          [ 6.9205e-02, -1.6116e-02, -5.2094e-02, -7.7957e-02,  2.1727e-02],\n",
      "          [-6.3890e-02,  7.7470e-02, -5.5377e-02, -2.0513e-02, -5.0972e-02],\n",
      "          [-7.4520e-02, -1.6498e-02,  1.0551e-02,  4.2977e-02,  3.1638e-02]],\n",
      "\n",
      "         [[ 4.2345e-02, -5.7852e-02, -3.0379e-02,  7.9480e-02, -4.5837e-03],\n",
      "          [ 3.8390e-02, -7.7046e-03,  5.8973e-03,  2.0271e-03, -3.5209e-02],\n",
      "          [ 7.6496e-02, -3.3645e-02, -1.0598e-02, -4.2234e-02, -4.9618e-02],\n",
      "          [ 7.8017e-02,  7.9670e-02, -2.3835e-02, -3.4378e-02,  4.6874e-02],\n",
      "          [ 7.1194e-02, -5.4172e-02,  1.6044e-02, -2.1029e-02, -4.5799e-02]],\n",
      "\n",
      "         [[-1.0989e-02,  1.2420e-02,  1.1305e-02,  5.9229e-02,  3.2040e-02],\n",
      "          [-1.7605e-02, -6.0244e-02, -1.4261e-02,  5.8695e-02,  4.2634e-02],\n",
      "          [ 1.0813e-02, -1.1932e-02, -2.1288e-02,  3.8746e-02,  5.1470e-02],\n",
      "          [-5.9717e-02, -1.4477e-02, -1.9468e-03,  5.1163e-02,  4.6085e-03],\n",
      "          [ 3.8743e-02,  3.5768e-02,  2.5792e-02, -2.7077e-02, -5.3993e-02]],\n",
      "\n",
      "         [[ 8.0118e-02,  7.4908e-02,  6.6675e-02,  7.1874e-02, -6.1108e-02],\n",
      "          [ 1.2441e-02,  3.1237e-02, -5.7624e-02,  3.0897e-02, -8.1232e-02],\n",
      "          [-5.9328e-02, -1.9446e-03, -2.9707e-03, -4.4377e-02,  5.4904e-02],\n",
      "          [ 7.8069e-02, -7.8380e-02, -7.8356e-02,  5.5625e-02,  7.0064e-02],\n",
      "          [ 4.2937e-02,  7.3410e-02, -2.2835e-02, -2.1718e-02, -6.6086e-02]],\n",
      "\n",
      "         [[ 6.2517e-02,  3.3856e-02, -4.7767e-02, -6.5874e-02,  2.8055e-03],\n",
      "          [ 7.9022e-02, -5.3007e-02, -5.3955e-02,  7.3532e-02,  6.1046e-02],\n",
      "          [-1.8380e-02, -7.0454e-02,  6.9815e-02, -1.8137e-02, -7.3878e-02],\n",
      "          [ 7.2356e-02,  4.9706e-02, -1.3588e-02,  7.3074e-02, -1.1737e-02],\n",
      "          [ 6.2844e-02,  6.4647e-02,  8.4100e-03, -7.8265e-02, -5.1216e-02]],\n",
      "\n",
      "         [[-5.8679e-03, -6.5202e-02,  7.8607e-02,  5.2072e-02,  8.9644e-03],\n",
      "          [-5.4205e-03, -5.6481e-02,  5.8882e-02,  4.0701e-03,  5.4107e-02],\n",
      "          [-1.9101e-02, -2.3532e-02, -6.8643e-02,  5.0060e-02,  1.3194e-02],\n",
      "          [ 4.9416e-02, -2.6552e-02, -4.5614e-02, -6.8967e-02,  7.4996e-02],\n",
      "          [-2.4106e-02, -4.9438e-02,  7.4356e-03, -5.2179e-02,  5.9550e-02]]]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0632,  0.0241,  0.0699, -0.0194,  0.0746,  0.0788, -0.0669, -0.0009,\n",
      "        -0.0777,  0.0377, -0.0557, -0.0661,  0.0090, -0.0352,  0.0414, -0.0626],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0012,  0.0321,  0.0399,  ..., -0.0139, -0.0458, -0.0118],\n",
      "        [-0.0148,  0.0103,  0.0149,  ..., -0.0014, -0.0494,  0.0415],\n",
      "        [-0.0128, -0.0080,  0.0421,  ...,  0.0091, -0.0447,  0.0434],\n",
      "        ...,\n",
      "        [-0.0077,  0.0360,  0.0137,  ...,  0.0334,  0.0090, -0.0281],\n",
      "        [-0.0061, -0.0075, -0.0062,  ...,  0.0237, -0.0091, -0.0250],\n",
      "        [ 0.0433, -0.0192, -0.0244,  ...,  0.0134,  0.0100, -0.0001]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-1.5684e-02,  4.5994e-02,  3.7032e-02,  4.1556e-02, -4.7080e-02,\n",
      "         1.9907e-02,  1.0892e-02,  2.4299e-02,  1.8525e-02,  3.2907e-02,\n",
      "        -1.6311e-02,  3.1126e-02, -3.3759e-02, -2.2831e-02, -1.3494e-02,\n",
      "         1.6986e-03,  3.8602e-02,  3.4898e-02,  1.4432e-03,  9.6410e-03,\n",
      "         1.8692e-02, -4.2460e-02, -1.2662e-02,  3.5427e-02,  1.5119e-02,\n",
      "        -1.0021e-02, -2.1625e-02, -1.6846e-02, -1.0308e-02, -1.4122e-02,\n",
      "        -3.2802e-03,  2.2590e-02,  4.0461e-02, -4.9289e-02, -4.4214e-02,\n",
      "         3.4859e-02,  3.9884e-02,  4.2665e-02, -2.3536e-02, -4.4786e-03,\n",
      "        -1.2234e-02,  2.9466e-02,  1.7778e-02,  2.9396e-02,  1.1861e-02,\n",
      "        -1.4497e-02, -1.5614e-03, -4.1905e-02, -5.6864e-03, -4.7728e-02,\n",
      "        -9.3724e-03, -4.8766e-02, -3.2113e-02,  4.1289e-02,  8.2921e-05,\n",
      "        -5.4318e-03, -5.4358e-03, -4.8384e-02,  3.9674e-02,  8.6126e-03,\n",
      "        -9.6798e-03, -3.3151e-02, -3.4365e-02,  7.1167e-03,  3.1821e-02,\n",
      "         4.1222e-02,  1.1749e-02, -2.4271e-02,  4.6152e-02,  2.2303e-02,\n",
      "        -2.8704e-02, -4.4361e-02,  3.1283e-02,  3.1540e-02,  5.9334e-03,\n",
      "        -8.7382e-03, -1.2112e-02, -2.9695e-02, -3.3753e-02, -2.9961e-02,\n",
      "         2.3150e-02, -2.1866e-02,  3.7211e-02,  3.3758e-02, -1.6594e-02,\n",
      "         3.1516e-03,  4.8681e-02, -2.4760e-02,  3.4275e-02, -3.6358e-02,\n",
      "        -1.8480e-02, -3.0846e-02,  4.7891e-02,  3.8405e-02,  2.0851e-02,\n",
      "         1.6766e-03, -9.2457e-03,  2.6416e-02, -4.4388e-02,  3.0886e-02,\n",
      "        -2.3066e-02, -2.0360e-02, -4.7815e-02, -2.1705e-02, -2.9613e-02,\n",
      "         4.0992e-02, -4.3615e-02, -4.5198e-02,  3.2748e-02, -7.4783e-03,\n",
      "        -8.2366e-03,  1.2356e-02, -3.5666e-02, -4.9923e-02, -2.5469e-02,\n",
      "        -2.0280e-02,  2.9516e-02, -1.5790e-03,  1.1562e-02,  4.4424e-02],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0129, -0.0869,  0.0471,  ..., -0.0444, -0.0289, -0.0728],\n",
      "        [-0.0340,  0.0436, -0.0132,  ...,  0.0537,  0.0266,  0.0698],\n",
      "        [ 0.0541, -0.0047, -0.0106,  ...,  0.0835,  0.0805,  0.0904],\n",
      "        ...,\n",
      "        [-0.0108, -0.0095,  0.0202,  ..., -0.0759,  0.0360, -0.0773],\n",
      "        [ 0.0749,  0.0452, -0.0664,  ..., -0.0578, -0.0493,  0.0246],\n",
      "        [ 0.0758, -0.0370, -0.0857,  ..., -0.0013, -0.0499, -0.0823]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0490,  0.0611,  0.0516, -0.0302, -0.0824, -0.0369,  0.0781, -0.0596,\n",
      "         0.0654,  0.0787,  0.0171, -0.0594, -0.0489,  0.0794,  0.0164, -0.0775,\n",
      "        -0.0628, -0.0112,  0.0794,  0.0323,  0.0470, -0.0733,  0.0396,  0.0700,\n",
      "        -0.0101,  0.0847, -0.0173, -0.0281,  0.0565, -0.0630, -0.0903,  0.0550,\n",
      "         0.0046,  0.0455, -0.0856, -0.0566,  0.0752,  0.0010, -0.0580, -0.0626,\n",
      "        -0.0397, -0.0228,  0.0013,  0.0254, -0.0430,  0.0676,  0.0203,  0.0747,\n",
      "        -0.0409,  0.0698, -0.0620,  0.0204, -0.0861, -0.0614, -0.0708, -0.0252,\n",
      "         0.0033,  0.0054, -0.0298,  0.0781,  0.0703, -0.0154,  0.0704,  0.0423,\n",
      "        -0.0784,  0.0169, -0.0309, -0.0111, -0.0108, -0.0097,  0.0017,  0.0200,\n",
      "         0.0048,  0.0778, -0.0295,  0.0236,  0.0292, -0.0061,  0.0251, -0.0471,\n",
      "         0.0393,  0.0437, -0.0264,  0.0601], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0699, -0.1032,  0.0096,  0.1087, -0.0781, -0.1041,  0.1088, -0.0482,\n",
      "          0.0653,  0.0432, -0.0618, -0.0201, -0.0018, -0.0918,  0.0028, -0.0871,\n",
      "         -0.0661,  0.0265,  0.0368,  0.0878,  0.0816, -0.0222,  0.0994,  0.0813,\n",
      "         -0.0518, -0.0726, -0.0284, -0.0858, -0.0842,  0.0799, -0.0091, -0.0407,\n",
      "          0.0383, -0.0360,  0.0839,  0.0762,  0.1030, -0.0911,  0.0762,  0.0904,\n",
      "          0.0665,  0.1028, -0.0059,  0.0907,  0.1074, -0.0389,  0.0203, -0.0472,\n",
      "         -0.0823, -0.1000, -0.0409, -0.0861,  0.0953,  0.1006, -0.0717,  0.0629,\n",
      "          0.0529,  0.0739, -0.0535, -0.0038,  0.0142, -0.0410,  0.0351, -0.0276,\n",
      "         -0.0569,  0.0595,  0.0533, -0.0655,  0.0620,  0.0111, -0.0734,  0.0333,\n",
      "          0.0226, -0.0495, -0.0391,  0.0736, -0.0395,  0.0226, -0.0684,  0.0787,\n",
      "          0.0667, -0.0666, -0.1023,  0.0473],\n",
      "        [ 0.1078, -0.0866,  0.0322, -0.0024,  0.0662, -0.0286, -0.0393,  0.0565,\n",
      "         -0.0558,  0.0069, -0.1080, -0.0152, -0.0774,  0.0870,  0.1060,  0.0696,\n",
      "          0.0613, -0.0302, -0.0533,  0.0772, -0.0061, -0.0058,  0.0700,  0.0714,\n",
      "         -0.0489, -0.0726,  0.0771,  0.0791, -0.0686,  0.0894,  0.0590,  0.0959,\n",
      "         -0.0416,  0.0784,  0.0298, -0.0318, -0.0731,  0.0770,  0.0635,  0.0584,\n",
      "          0.0138,  0.0783, -0.0674,  0.0579,  0.0430, -0.0517, -0.1060, -0.0945,\n",
      "         -0.0077, -0.0209, -0.0532, -0.0224,  0.1027, -0.0773, -0.0659,  0.1010,\n",
      "         -0.0252, -0.0886,  0.0346, -0.0961,  0.0149,  0.0569,  0.0525, -0.0579,\n",
      "          0.1052,  0.1032, -0.0620,  0.0769,  0.0399,  0.1009, -0.0182, -0.0545,\n",
      "         -0.0910,  0.0700,  0.0190, -0.1087, -0.0071, -0.0607,  0.0955,  0.0941,\n",
      "         -0.1033,  0.0637,  0.0016, -0.0943],\n",
      "        [-0.1034,  0.0476,  0.0367,  0.0980, -0.0260,  0.0586,  0.0845,  0.0215,\n",
      "         -0.0030, -0.0548, -0.0791, -0.0933, -0.0172,  0.0503, -0.0910, -0.0651,\n",
      "          0.0273, -0.0077,  0.0968, -0.0156,  0.0704, -0.0532, -0.0202,  0.0038,\n",
      "          0.0131,  0.0749, -0.0269,  0.0745,  0.0817,  0.0047,  0.0182, -0.0776,\n",
      "          0.0936, -0.0162,  0.0427, -0.0958, -0.0960, -0.0567,  0.0183,  0.0426,\n",
      "         -0.0388,  0.0962, -0.1059,  0.0824,  0.0438,  0.0754, -0.0012,  0.0409,\n",
      "          0.0831,  0.0844, -0.0148,  0.0551, -0.0877, -0.0830,  0.0314, -0.0619,\n",
      "          0.1062, -0.0205,  0.0560, -0.1082,  0.0909, -0.0740, -0.0608,  0.0818,\n",
      "         -0.0764,  0.0949,  0.0991, -0.0844, -0.1032, -0.1030,  0.0672, -0.0153,\n",
      "         -0.1014, -0.0063, -0.0575, -0.0049,  0.0299,  0.0178, -0.0441, -0.1006,\n",
      "         -0.0229, -0.0535, -0.0674, -0.0656],\n",
      "        [-0.0926,  0.0752,  0.0525,  0.0715,  0.0354,  0.1024,  0.0525,  0.0437,\n",
      "         -0.0051, -0.0583,  0.0010, -0.0088, -0.0901,  0.0857, -0.0090,  0.0804,\n",
      "         -0.0309, -0.0333, -0.0960, -0.0431, -0.0452, -0.0329, -0.0739, -0.0421,\n",
      "          0.0442, -0.0402,  0.0600, -0.0122, -0.0992,  0.0250,  0.0205, -0.0732,\n",
      "          0.0653, -0.0697, -0.0208, -0.0507, -0.0048,  0.0940,  0.0675,  0.0996,\n",
      "          0.1083, -0.0983,  0.0205, -0.0873, -0.0925,  0.0095,  0.1086, -0.0338,\n",
      "          0.1033,  0.0388, -0.0826,  0.0377, -0.0188, -0.0546, -0.0735,  0.0884,\n",
      "          0.0987,  0.0233, -0.0989, -0.0183,  0.0704, -0.0852, -0.1003, -0.1026,\n",
      "          0.0684,  0.0280, -0.0231,  0.1079,  0.0800, -0.0795,  0.0814, -0.1035,\n",
      "          0.0082,  0.1011,  0.0210, -0.0525, -0.0884, -0.0765, -0.0464, -0.0654,\n",
      "          0.0015,  0.0215, -0.0440,  0.0738],\n",
      "        [-0.1036, -0.0958,  0.0693, -0.0059,  0.0977, -0.0918,  0.0395,  0.0445,\n",
      "          0.0825, -0.0868,  0.0445,  0.1043,  0.0999,  0.0223,  0.0784, -0.0516,\n",
      "          0.0556, -0.0074,  0.0061,  0.1005,  0.0888, -0.0503,  0.0327,  0.0419,\n",
      "          0.0828, -0.0282,  0.0821, -0.0996,  0.0007, -0.0974, -0.1088,  0.0132,\n",
      "          0.0503,  0.0543,  0.0657,  0.0916, -0.0322,  0.0435, -0.0662, -0.0958,\n",
      "          0.0275, -0.0310, -0.0366, -0.0846,  0.0866,  0.0886, -0.0199,  0.0229,\n",
      "         -0.0373,  0.0760, -0.0473,  0.0985, -0.0107,  0.0843,  0.0682,  0.0726,\n",
      "          0.0899, -0.0419,  0.0487,  0.0909,  0.0771,  0.0395,  0.0138, -0.0702,\n",
      "         -0.0391, -0.0989, -0.0216, -0.0158,  0.0019, -0.0199,  0.0716, -0.1073,\n",
      "         -0.0640, -0.0421,  0.0926,  0.0690, -0.0801, -0.0148, -0.1012, -0.0855,\n",
      "          0.1086, -0.0978, -0.0597,  0.1029],\n",
      "        [ 0.0752, -0.0397,  0.0686,  0.0472, -0.0317,  0.0181, -0.1081, -0.0817,\n",
      "          0.0787, -0.0201, -0.0744, -0.0678,  0.0786, -0.0807, -0.0102,  0.0568,\n",
      "          0.0381, -0.0317,  0.0139, -0.1024, -0.0826,  0.0116,  0.0358, -0.0880,\n",
      "          0.0449, -0.1009,  0.0253, -0.0127, -0.0852, -0.0204, -0.0832, -0.0799,\n",
      "          0.0058,  0.0754, -0.0492, -0.0648,  0.0644, -0.0098,  0.0427, -0.0107,\n",
      "         -0.0679,  0.0603, -0.0222, -0.0498,  0.0994, -0.0743, -0.0095, -0.0705,\n",
      "          0.0284, -0.0953,  0.0339,  0.0857, -0.0850,  0.0232, -0.0112, -0.0749,\n",
      "          0.0227,  0.0442, -0.0848, -0.0976,  0.0555,  0.0946,  0.0772,  0.0617,\n",
      "         -0.0813, -0.0148, -0.0921, -0.0186, -0.0734,  0.0977, -0.0710,  0.0508,\n",
      "          0.0864, -0.0690,  0.0929, -0.0658,  0.0212,  0.0389, -0.0912,  0.0338,\n",
      "         -0.1030, -0.0975,  0.0833,  0.0106],\n",
      "        [-0.0915, -0.0142, -0.0192, -0.0232,  0.0154,  0.0846, -0.0612, -0.0197,\n",
      "          0.0465, -0.0925, -0.0797, -0.0023,  0.0022,  0.0355,  0.0662,  0.0115,\n",
      "          0.0745, -0.0551,  0.0435, -0.0405, -0.1002, -0.0493,  0.0606, -0.0289,\n",
      "         -0.0166, -0.0224, -0.0805, -0.0057,  0.0868,  0.0390, -0.0917,  0.0504,\n",
      "         -0.1031, -0.0732, -0.0592, -0.0378,  0.0259,  0.0289,  0.0394,  0.1079,\n",
      "          0.0504, -0.0761,  0.0306,  0.0624,  0.0517,  0.0016,  0.0017, -0.0182,\n",
      "          0.0374, -0.0793, -0.1038, -0.0736,  0.0076,  0.0209,  0.0057, -0.0479,\n",
      "          0.0265,  0.1058,  0.0870, -0.0134,  0.0493, -0.0711, -0.0807,  0.0473,\n",
      "         -0.0990, -0.0923, -0.0019,  0.0166, -0.1000,  0.0131,  0.0955, -0.0710,\n",
      "          0.0658, -0.0264, -0.0765,  0.0212,  0.0982, -0.0295, -0.0620, -0.0786,\n",
      "          0.0466,  0.0780,  0.0998, -0.0846],\n",
      "        [-0.0307,  0.0558, -0.0476,  0.0313, -0.0465, -0.1062, -0.0589,  0.0175,\n",
      "         -0.0177, -0.0835, -0.0203, -0.0628, -0.1005, -0.0800, -0.1076, -0.0775,\n",
      "          0.0793, -0.0557, -0.0777, -0.0741, -0.1004,  0.0104,  0.0038,  0.0760,\n",
      "         -0.0896,  0.0056, -0.0697, -0.0440,  0.0208,  0.0635, -0.0540,  0.0241,\n",
      "          0.0117,  0.0726,  0.0476, -0.0931, -0.0308,  0.0014,  0.0773,  0.1020,\n",
      "          0.0122,  0.0114,  0.0124,  0.0937, -0.0879, -0.1031, -0.0903,  0.0692,\n",
      "          0.0766, -0.0402,  0.0343, -0.1084, -0.0139,  0.0025, -0.0022, -0.0269,\n",
      "         -0.1029,  0.0248, -0.0159, -0.0626,  0.0838,  0.0287,  0.0154, -0.1005,\n",
      "          0.0907,  0.1082,  0.0913, -0.0560, -0.0448,  0.0032,  0.0972, -0.0140,\n",
      "         -0.0509, -0.0291,  0.0777,  0.0747, -0.0458, -0.1054,  0.0021, -0.0492,\n",
      "         -0.0777,  0.0850, -0.0463,  0.0324],\n",
      "        [ 0.0621, -0.0673, -0.0349, -0.0248,  0.0501,  0.0537,  0.0910, -0.0903,\n",
      "         -0.0842,  0.0147, -0.0383, -0.0912,  0.0534,  0.0375, -0.0569,  0.0336,\n",
      "         -0.0320,  0.0109,  0.0251,  0.0354,  0.0653, -0.0473, -0.0743, -0.0546,\n",
      "         -0.0495, -0.0801,  0.0149,  0.0552, -0.0502, -0.0183,  0.0395, -0.0732,\n",
      "         -0.0956, -0.0477, -0.0778,  0.0189,  0.0381,  0.0523,  0.0225, -0.0456,\n",
      "         -0.0371,  0.0811,  0.0026,  0.0783,  0.0816,  0.0465, -0.1001,  0.0691,\n",
      "         -0.0707, -0.0774,  0.0361, -0.0588,  0.1035,  0.0623, -0.0425,  0.0757,\n",
      "         -0.0006, -0.0667,  0.0535, -0.0016, -0.0996,  0.0459,  0.0103,  0.0502,\n",
      "          0.0056,  0.0497, -0.0753, -0.0063,  0.0777,  0.1024, -0.0261, -0.0626,\n",
      "         -0.0770, -0.0451, -0.0386,  0.1004,  0.0609,  0.0522,  0.0011, -0.0638,\n",
      "         -0.0442, -0.0269, -0.0366, -0.0408],\n",
      "        [ 0.1059, -0.1038, -0.0515,  0.0877, -0.0570, -0.0870,  0.0106,  0.0659,\n",
      "          0.0230, -0.0117,  0.0955, -0.0214, -0.0651,  0.0623, -0.0678,  0.0439,\n",
      "         -0.0918, -0.0832, -0.0267,  0.0607,  0.0446, -0.0920,  0.0762, -0.0433,\n",
      "         -0.0246, -0.0799,  0.0106,  0.0915,  0.1043,  0.0538,  0.1077,  0.1005,\n",
      "         -0.0294,  0.0958, -0.0798,  0.0287,  0.0236, -0.0545, -0.0730, -0.0583,\n",
      "         -0.0647,  0.0383, -0.0515, -0.0095,  0.0771,  0.1000,  0.0294, -0.0139,\n",
      "         -0.0635,  0.0391,  0.0918, -0.1036,  0.0989, -0.0046, -0.0667,  0.0696,\n",
      "         -0.0010,  0.1016,  0.0528,  0.0485, -0.0827, -0.0034, -0.0210,  0.0918,\n",
      "          0.0348, -0.0834,  0.0271,  0.0351,  0.0813, -0.0547, -0.0816,  0.1041,\n",
      "          0.0434, -0.0836,  0.0619,  0.0796, -0.0781, -0.0896, -0.0488, -0.0445,\n",
      "          0.0388, -0.0995, -0.0350,  0.0829]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0045, -0.0773,  0.0553, -0.0169,  0.1017,  0.0645, -0.0102, -0.0622,\n",
      "         0.1049, -0.0376], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try a random 32x32 input\n",
    "Note: Expected input size to this net(LeNet) is 32x32. To use this net on\n",
    "MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0400, -0.1663,  0.0861,  0.0221,  0.1611, -0.0145,  0.0184, -0.1475,\n",
      "          0.0923, -0.0208]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c22752f93b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/xzz/anaconda2/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xzz/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "    package only supports inputs that are a mini-batch of samples, and not\n",
    "    a single sample.\n",
    "\n",
    "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "    a fake batch dimension.</p></div>\n",
    "\n",
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation, creates at\n",
    "     least a single ``Function`` node, that connects to functions that\n",
    "     created a ``Tensor`` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered:**\n",
    "  -  Defining a neural network\n",
    "  -  Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "  -  Computing the loss\n",
    "  -  Updating the weights of the network\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "`loss functions <http://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target.\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6274, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its\n",
    "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
    "like this:\n",
    "\n",
    "::\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n",
    "will have their ``.grad`` Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fc9af215c90>\n",
      "<AddmmBackward object at 0x7fc9af215350>\n",
      "<AccumulateGrad object at 0x7fc9af340fd0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0091, -0.0119, -0.0039,  0.0080,  0.0015, -0.0152])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is `here <http://pytorch.org/docs/nn>`_.\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network\n",
    "\n",
    "Update the weights\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple python code:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Note::\n",
    "\n",
    "      Observe how gradient buffers had to be manually set to zero using\n",
    "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
    "      as explained in `Backprop`_ section.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
